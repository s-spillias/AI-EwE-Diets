\subsection{Validation Framework}

We executed model generation across three distinct phases. In phase one, we established baseline configurations for each study region by processing species occurrence data and research parameters. We terminated this phase prior to species grouping, creating standardized input states for subsequent validation iterations.

We executed five independent iterations per region in phase two. Each iteration began with species grouping and proceeded through the complete model construction sequence. We maintained fixed input parameters across iterations while allowing the AI's stochastic decision processes to generate natural variation in outputs.

\subsubsection{Study Regions}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/validation_regions.pdf}
    \caption{Map of the three validation regions used in this study: Northern Territory (orange), South East shelf (blue), and South East Offshore (green).}
    \label{fig:validation_regions}
    \end{figure}

We validate our framework using three Australian marine regions that present distinct ecological characteristics and modeling challenges (Figure \ref{fig:validation_regions}). The Northern Territory region represents a tropical ecosystem characterized by complex reef systems, seasonal monsoon influences, and high biodiversity. This region tests the framework's ability to handle diverse species assemblages and complex trophic interactions in a dynamic environment.

The South East shelf region represents a temperate coastal system with extensive ecological records. This region has comprehensive diet information in established databases, well-documented EwE models spanning multiple years, and active research programs. The rich availability of expert knowledge and historical data makes this region ideal for validating the framework's data integration capabilities.

The South East Offshore region presents a deep-water ecosystem that challenges the framework with data-limited conditions and unique ecological patterns. This region tests the framework's capacity to handle situations where direct observational data may be sparse and where species interactions may be less well understood. The contrasting characteristics of these three regions provide a robust test of the framework's adaptability across different ecological contexts.


\subsubsection{Group Consistency Analysis}

To validate the ecological validity of AI-generated species groupings, we developed quantitative measures of grouping consistency. We tracked each species' group assignments across iterations and calculated a consistency score:

\[
\text{Consistency Score} = \frac{\text{Number of occurrences in most common group}}{\text{Total number of iterations}}
\]

This metric quantifies the framework's decision-making reliability for individual species. We classified species with consistency scores below 0.95 as unstable, indicating variable group assignments across iterations. Chi-square tests on these consistency scores help identify whether grouping decisions remain stable across different ecological contexts, addressing a key aspect of the framework's reliability.

To evaluate broader patterns in group formation, we assessed group stability using the Jaccard similarity coefficient between consecutive iterations:

\[
J(i,j) = \frac{|M_{i} \cap M_{j}|}{|M_{i} \cup M_{j}|}
\]

where $M_{i}$ and $M_{j}$ represented species members in iterations $i$ and $j$. We calculated the overall stability score by averaging Jaccard similarities across consecutive iteration pairs. This approach, combined with coefficients of variation analysis, reveals how consistently the framework identifies and maintains ecologically meaningful groupings across different runs. One-way ANOVA tests on these stability measures across regions, supplemented with Cohen's f effect size calculations, demonstrate the framework's adaptability to different marine ecosystems while maintaining consistent decision-making patterns.

\subsubsection{Diet Matrix Analysis}
To evaluate the reliability of AI-generated trophic interactions and assess the framework's ability to capture distinct ecological patterns, we developed a multi-metric analysis approach. We focused on significant predator-prey interactions, defined as those comprising more than 5\% of a predator's diet, using both descriptive statistics and correlation analyses. For each interaction, we calculated:

\begin{enumerate}
    \item Presence ratio across iterations:
    \[
    P_{ij} = \frac{\text{Number of iterations with interaction}}{n}
    \]
    where $n$ was the total number of iterations.

    \item Mean diet proportion:
    \[
    \mu_{ij} = \frac{1}{n}\sum_{k=1}^{n} x_{ijk}
    \]
    where $x_{ijk}$ represents diet proportion for predator $i$ consuming prey $j$ in iteration $k$.

    \item Stability score:
    \[
    S_{ij} = \frac{1}{n}\sum_{k=1}^{n} \frac{|x_{ijk} - \mu_{ij}|}{\max_{k}(x_{ijk})}
    \]
    where $x_{ijk}$ represents the diet proportion for predator $i$ consuming prey $j$ in iteration $k$, $\mu_{ij}$ is the mean diet proportion across iterations, and $\max_{k}(x_{ijk})$ is the maximum value across iterations. This score ranges from 0 (perfect stability) to 1 (maximum instability), with lower values indicating more consistent diet proportions across iterations.
\end{enumerate}

We chose this stability metric over traditional variance measures for several reasons. First, by normalizing deviations by the maximum value, the metric achieves scale independence, allowing meaningful comparisons between interactions of different magnitudes. For example, the sequences [0.2, 0.2, 0.2, 0.2, 0.1] and [0.02, 0.02, 0.02, 0.02, 0.01] would yield the same stability score despite having different absolute variances. Second, the bounded range between 0 and 1 provides an intuitive scale for assessing stability, unlike the unbounded nature of variance. Third, this approach effectively handles small variations in small values, ensuring that minor fluctuations in trace diet components do not disproportionately influence the stability assessment.

We classified interactions as unstable when their stability score exceeded 0.3, indicating significant variation in diet proportions across iterations. To illustrate this metric:

\begin{itemize}
    \item A stable interaction (S = 0.08) might show values [0.02, 0.02, 0.02, 0.02, 0.01], where proportions remain very similar across iterations
    \item An unstable interaction (S = 0.39) might show values [0.027, 0.25, 0.25, 0.067, 0.25], where proportions vary substantially between iterations
\end{itemize}

This metric provides a continuous measure of stability that handles both presence/absence patterns and magnitude variations in a unified way, avoiding discontinuities at zero values. To assess the framework's ability to capture distinct ecological patterns across regions, we employed pairwise Spearman correlations between iterations to evaluate the consistency of predator-prey relationships, focusing on significant interactions. This non-parametric approach accounts for the potentially non-normal distribution of diet proportions. We supplemented this with Kruskal-Wallis tests to identify significant differences in trophic structure across regions, providing evidence of the framework's ability to distinguish unique ecological characteristics in different marine ecosystems.

%\subsubsection{Expert Evaluation Protocol}
%To evaluate the quality of diet matrices generated through artificial intelligence methods, we implemented a single-blind validation framework comparing AI-generated and expert-created Ecopath with Ecosim (EwE) diet matrices across three distinct regions. Two independent domain experts were engaged to assess both matrix types without knowledge of their origin. The evaluation protocol employed a five-point Likert scale assessment, where a score of 1 indicated strong preference for the AI-generated matrix and 5 indicated strong preference for the expert-generated matrix. 

%The validation framework encompassed two primary assessment criteria. First, experts evaluated the structural validity of functional group classifications within each matrix. Second, they assessed the ecological plausibility of diet proportions assigned to each functional grouping. To capture nuanced insights beyond numerical scores, experts provided qualitative feedback on the strengths and weaknesses of each matrix. This dual quantitative-qualitative approach enabled comprehensive evaluation of both methodological approaches to diet matrix construction.
